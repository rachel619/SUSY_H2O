---
title: "STAT418 Assignment 3"
author: "Rui Qiao"
date: "5/22/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## SUSY Data Set
Detailed description: https://archive.ics.uci.edu/ml/datasets/SUSY

"This is a classification problem to distinguish between a signal process which produces supersymmetric particles and a background process which does not." 

"The first column is the class label (1 for signal, 0 for background), followed by the 18 features (8 low-level features then 10 high-level features). The first 8 features are kinematic properties measured by the particle detectors in the accelerator. The last ten features are functions of the first 8 features; these are high-level features derived by physicists to help discriminate between the two classes."

The original data file ("SUSY.csv") contains 5,000,000 observations and has file size of 2.39 GB. Uploading that file to the H2O cluster will generate an error (java.lang.OutOfMemoryError: Java heap space). H2O recommends that the memory should be four times the size of the data. Since the memory of my computer is 8GB, I decided to use a much smaller dataset. 

I want to reduce the observations to 250,000. Besides deleting observations, I chose to delete all the low-level features in the dataset, leaving only high-level features as the predictor variables and the class label as the response. (An alternative way here to reduce file size can be spliting the file with 80% of observations.)

New file was produced and checked by running following codes in terminal (MacOS).
> cd Desktop/STAT418/SUSY
> cut -d, -f1,10- SUSY.csv > SUSY_high.csv
> split -l 500000 SUSY_high.csv SUSY_high_

This gives me a file named as "SUSY_high_aa" with 250,000 rows and 11 columns.

## Data Loading and Preparation

```{r data}
# h2o.shutdown()
library(h2o)

h2o.init(nthreads = -1)

col_names <- c("class", "MET_rel", "axial MET", "M_R", "M_TR_2", "R", 
               "MT2", "S_R", "M_Delta_R", "dPhi_r_b", "cos(theta_r1)")

SUSY <- h2o.importFile("/Users/Rachel/Desktop/STAT418/SUSY/SUSY_high_aa",
                       destination_frame = "SUSY", col.names = col_names)

y <- "class"
x <- setdiff(names(SUSY),y)

# For binary classification, response should be a factor
SUSY[y] <- as.factor(SUSY[y])
summary(SUSY[y])
# 1 for signal, 0 for background

# Split Data: train-valid-test
# Do not use cross-validation, dataset is large enough
parts <- h2o.splitFrame(SUSY, c(0.8,0.1), seed = 123)
SUSY_train <- parts[[1]]
SUSY_valid <- parts[[2]]
SUSY_test <- parts[[3]]
rm(parts)

```

## Logistic Regression (LR)

Logistic Regression (binomial classification): family = biomial

Lasso regularization: alpha = 1.0

Try various values for lambda.

```{r}
system.time({
  model_LR <- h2o.glm(x, y, training_frame = SUSY_train,
                      validation_frame = SUSY_valid,
                      family = "binomial",alpha = 1.0, 
                      lambda = c(1, 0.5, 0.1, 0.01, 0.001, 0.0001,
                                 0.00001, 0.000001, 0.0000001, 0),
                      lambda_search = TRUE)
})


h2o.auc(h2o.performance(model_LR, SUSY_valid))

model_LR

s_1 <- h2o.auc(h2o.performance(model_LR, SUSY_test))

s1 <- h2o.auc(h2o.performance(model_LR, SUSY_test))
s1
```
Among all the LR models. lambda = 0.0 will give largest auc for the validation dataset. The final choosen LR model has auc = 0.8327407 on test data set.


## Random Forest (RF)

Hyperparameters to try:
1. ntrees = c(30, 50)

2. max_depth = c(10, 20)

3. mtries = c(3,5)
```{r RF}
hyper_params <- list( ntrees = c(20,50), max_depth = c(10, 20), mtries = c(3,5))

RF_grid <- h2o.grid("randomForest",x = x, y = y, training_frame = SUSY_train, 
                 validation_frame = SUSY_valid, nfolds = 0,
                 grid_id = "SUSY_RF_grid", hyper_params = hyper_params,
                 search_criteria = list(strategy = "Cartesian"),
                 stopping_metric = "AUC", stopping_tolerance = 0, stopping_rounds = 3)

RF_auc <- h2o.getGrid(RF_grid@grid_id, sort_by="auc", decreasing = TRUE)
RF_auc
range(RF_auc@summary_table$auc)

```

Eight models tested. The highest auc is RF with max_depth = 20, mtries = 3, ntrees =50.

```{r}
system.time({
  model_RF <- h2o.randomForest(x = x, y = y, training_frame = SUSY_train, 
                 validation_frame = SUSY_valid, nfolds = 0,
                 max_depth = 20, mtries = 3, ntrees =50)
})

model_RF

s_2 <- h2o.auc(h2o.performance(model_RF, SUSY_test))

s2 <- h2o.auc(h2o.performance(model_RF, SUSY_test))
s2
```


## Gradient Boosting Model (GBM)
1. max_depth = c(5, 10)

2. learn_rate = c(0.1, 0.2)

3. early stopping for determining the number of trees

(If not using early stopping and overtraining (increasing the number of iterations/trees), the process time will be too large. In fact, I originally do GBM without early stopping with a larger dataset. It took a great amount of running time and H2O ended up not having enough memory to run all the codes.)
```{r GBM}
hyper_params = list(max_depth = c(5, 10),learn_rate = c(0.1, 0.2))

GBM_grid <- h2o.grid("gbm",x = x, y = y, training_frame = SUSY_train, 
                 validation_frame = SUSY_valid, nfolds = 0,
                 grid_id = "SUSY_GBM_grid", hyper_params = hyper_params,
                 search_criteria = list(strategy = "Cartesian"),
                 stopping_tolerance = 0.02, stopping_rounds = 3, 
                 score_tree_interval = 10
)

GBM_auc <- h2o.getGrid(GBM_grid@grid_id, sort_by="auc", decreasing = TRUE)
GBM_auc
range(GBM_auc@summary_table$auc)
```

Four models tested. The highest auc is 0.8645064947023698, with learn_rate = 0.1, max_depth = 1.

```{r}
system.time({
  model_GBM <- h2o.gbm(x = x, y = y, training_frame = SUSY_train, 
                 validation_frame = SUSY_valid, 
                 max_depth = 1, learn_rate = 0.1)    
})


model_GBM

s_3 <- h2o.auc(h2o.performance(model_GBM, SUSY_test))

s3 <- h2o.auc(h2o.performance(model_GBM, SUSY_test))
s3
```
The final GBM give auc = 0.8254005 on test data.

## AUC/ ROC Comparation
```{r}

a <-as.data.frame(cbind(c("LR", "RF", "GBM"),c(s_1,s_2,s_3),c(s1,s2,s3)))
names(a) <- c("Model", "AUC on valid", "AUC on test")
a

```
Based on AUC comparison. Random Forest model with max_depth = 20, mtries = 3, ntrees =50 is the best model.


```{r LR-ROC, echo = FALSE}

plot(h2o.performance(model_LR, SUSY_test))

plot(h2o.performance(model_RF, SUSY_test))

plot(h2o.performance(model_GBM, SUSY_test))
```

Notice: Need to take a further look at the GBM (ROC curve). Continue...
